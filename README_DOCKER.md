# Docker : Build et Ex√©cution GPU (Ubuntu 24.04 + CUDA 13.1)

Ce d√©p√¥t contient un `Dockerfile` multi-stage optimis√© pour construire une image performante incluant **OpenCV 4.13.0 (CUDA)**, **PyTorch 2.9.1**, et **TensorRT 10.14**.

## üèóÔ∏è Proc√©dure de Build

Le build est optimis√© via un syst√®me multi-stage (OpenCV -> D√©pendances -> Runtime).

```bash
# Lancer le build (inclut la gestion du cache et le backup automatique)
./build_image.sh
```

### üîç V√©rification du build
Une fois l'image cr√©√©e, v√©rifiez que le GPU est bien accessible :
```bash
docker run --rm --gpus all people-counter:gpu-final python3 -c "import cv2; import torch; print('OpenCV CUDA:', cv2.cuda.getCudaEnabledDeviceCount()); print('PyTorch CUDA:', torch.cuda.is_available())"
```

## üß± Architecture CPU / GPU
```mermaid
flowchart LR
   subgraph Host[H√¥te Docker]
      CameraUSB -->|USB| CaptureService(Capture Process)
      CameraIP -->|RTSP/HLS| CaptureService
      CaptureService -->|M√©tadonn√©es| Pipeline
   end

   subgraph Pipeline["Application (containeur)"]
      Pipeline --> CPUQueue["CPU Pr√©-traitement"]
      CPUQueue --> GPUInfer["TensorRT YOLO / LWCC"]
      GPUInfer --> CPUDraw["CPU Fusion / Dessin"]
      CPUDraw --> WebUI["Web UI Metrics"]
   end

   subgraph Devices[Mat√©riel]
      GPU["GPU (CUDA, TensorRT)"]
      CPU["CPU (Linux host)"]
      GPUInfer --> GPU
      CPUQueue --> CPU
      CPUDraw --> CPU
      WebUI -->|HTTP| Browser
   end

   style GPUInfer stroke:#b83232,stroke-width:3px
   style GPU fill:#b83232,color:#fff
   style CPU fill:#2d5f8b,color:#fff
```

## üéûÔ∏è Flux vid√©o par tuiles
```mermaid
sequenceDiagram
   participant C as Capture
   participant P as Pipeline
   participant Y as YOLO Seg
   participant GPU as GPU
   participant D as Draw/UI

   C->>P: frame
   P->>Y: split into tiles
   Y->>Y: t_crop (CPU)
   Y->>GPU: t_inf (TensorRT)
   Y->>Y: t_fuse & t_draw (CPU)
   Y->>P: merged masks
   P->>D: overlay + metrics (includes t_inf, t_draw, total_internal)
```

## üß± Tiling + pr√©/post-traitement (4K vs 1080p)

La version 4K d√©coupe les 3840√ó2160 pixels en un *global tile* 640√ó640 (pour conserver la vue d'ensemble) et en une grille d'√Ælots 640√ó640 r√©partis sur la sc√®ne (8 √† 12 tiles selon le niveau de zoom). Le *global tile* sert deux objectifs cl√©s : il fournit une vue native pour la fusion finale (les masques des sous-tiles sont recoll√©s sur ce canevas global) et il agit de r√©f√©rence pour d√©terminer les offsets/scales des tiles locales. En 1080p, la grille reste plus compacte (4-6 tiles) mais le pattern est identique.

Chaque tile (global ou local) est copi√©e en m√©moire CPU, ses m√©tadonn√©es (position/√©chelle) sont stock√©es, puis la s√©quence est envoy√©e au moteur TensorRT pour √™tre trait√©e. Les 45‚ÄØms du GPU repr√©sentent la somme des inf√©rences sur toutes les tiles locales + le global tile, pas un ¬´‚ÄØpassage 4K‚ÄØ¬ª en un seul batch. Dans l‚Äôimpl√©mentation actuelle, le recadrage, la mise √† l‚Äô√©chelle et la reconstruction des masques se font sur CPU, ce qui explique les 195‚ÄØms de bout en bout.

Pour optimiser, il faut d√©placer le plus de ces √©tapes sur CUDA : activer `YOLO_USE_GPU_PREPROC=1` permet de travailler sur des buffers GPU avec un pipeline de kernels (resize, crop, copy) et `YOLO_USE_GPU_POST=1` donne √† TensorRT les moyens de dessiner les masques directement sur la texture globale. Un pipeline CUDA typique ici d√©finirait des streams d√©di√©s pour (1) charger l‚Äôimage globale dans un buffer, (2) produire une version downscal√©e (global tile) et des crops pour chacun des tiles locaux, (3) lancer les inf√©rences TensorRT en parall√®le via un ¬´‚ÄØbatch‚ÄØ¬ª compos√© de ces tiles, et enfin (4) fusionner les r√©sultats dans un buffer de sortie que la couche CPU/visu peut alpha-blender. Les streams CUDA permettent d‚Äôoverlapper copie/inf√©rence/post-processing pour √©viter les goulets d‚Äô√©tranglement.

En compl√©ment, on peut brider les classes d√©tect√©es sur l‚Äôindex 0 (personne) pour √©viter de diluer le moteur sur des cat√©gories inutiles. La fusion des masques elle-m√™me peut √™tre d√©l√©gu√©e √† un fragment shader OpenGL ou CUDA (voir `YOLO_USE_GPU_POST=1` / un shader simple) pour faire le blend sur le canevas global sans repasser en CPU.

```mermaid
flowchart TB
   Capture4K["Capture 4K / 3840√ó2160"] --> Crop4K["CPU crop & tile (global + 9+ tiles √† 640)"]
   Capture1080["Capture 1080p / 1920√ó1080"] --> Crop1080["CPU crop & tile (global + 4-6 tiles √† 640)"]
   Crop4K --> Upload4K["Staging CPU ‚Üí GPU (async batches)"]
   Crop1080 --> Upload1080["Staging CPU ‚Üí GPU (fewer tiles)"]
   Upload4K --> TensorRT["TensorRT (batch 32 tiles max)"]
   Upload1080 --> TensorRT
   TensorRT --> CPUFuse["Fusion et dessin de masques (CPU)"]
   CPUFuse --> WebMetrics["Web UI + m√©triques t_inf / t_draw"]
   style TensorRT stroke:#b83232,stroke-width:3px
   style CPUFuse stroke:#2d5f8b,stroke-width:2px
```

Le point d'optimisation principal est donc de r√©duire le temps pass√© sur le CPU¬†: activer `YOLO_USE_GPU_PREPROC=1` et/ou `YOLO_USE_GPU_POST=1` permet de d√©porter la resize/crop et la fusion vers des kernels CUDA (en conjonction avec la version TensorRT native). D'autres pistes¬†: groupement plus agressif des tiles dans des batches TensorRT plus larges, mise en pr√©-charge des copies via des streams CUDA d√©di√©s, ou bien d√©l√©gation de la fusion des masques √† un fragment shader (OpenGL/CUDA) pour √©viter les copies sur l'image finale.

## üß∞ Profils et fichiers `.env`

`run_app.sh` charge `scripts/configs/<profil>.env` pour injecter des `export KEY=VALUE` dans l'environnement Docker. Chaque fichier de profil d√©finit notamment `YOLO_BACKEND`, `YOLO_MODEL`, `YOLO_DEVICE`, `DEBUG_TILING` et, depuis peu, les variables `YOLO_USE_GPU_PREPROC` / `YOLO_USE_GPU_POST` qui d√©clenchent les pipelines GPU. Pour les profils bas√©s sur RTX (ex. `rtx_extreme.env`), ajoutez simplement¬†:

```
export YOLO_USE_GPU_PREPROC=1
export YOLO_USE_GPU_POST=1
export YOLO_BACKEND=tensorrt_native
export YOLO_DEVICE=cuda
```

Le script supprime les commentaires, source le fichier, puis lance `docker run` avec les variables export√©es ; il n‚Äôest donc pas n√©cessaire de d√©clarer ces exports ailleurs. Apr√®s modification, relancez `./run_app.sh --profile rtx_extreme` pour voir les changements. Vous pouvez v√©rifier les param√®tres appliqu√©s en d√©marrant un shell dans le conteneur (`./run_app.sh --profile rtx_extreme bash`) et en tapant `printenv | grep YOLO`.

Les variables `YOLO_USE_GPU_PREPROC` et `YOLO_USE_GPU_POST` respectent la m√™me logique que les autres : toute valeur non vide active la version CUDA, et elles sont h√©rit√©es par `camera_app_pipeline.py` √† travers les modules `yolo_seg_people_counter.py` / `yolo_people_counter.py`. La carte ¬´¬†YOLO internal (ms)¬†¬ª dans l‚Äôinterface Web affichera alors les temps d‚Äôinf√©rence r√©els, y compris les gains √©ventuels si les kernels CUDA sont charg√©s.

## üöÄ Ex√©cution de l'application

Utilisez le script d'ex√©cution qui g√®re automatiquement les acc√®s GPU, cam√©ras et ports r√©seaux.

```bash
# Lancer l'application (utilise /dev/video0 par d√©faut)
./run_app.sh

# Pour utiliser un autre p√©riph√©rique cam√©ra
./run_app.sh /dev/video1
```

---

## üì∏ Cam√©ra USB sur WSL2 (Windows)

Puisque le noyau WSL2 par d√©faut ne supporte pas les cam√©ras USB nativement (pas de `/dev/video*`), nous utilisons un **Bridge Vid√©o** pour envoyer le flux de Windows vers Docker.

### 1. Sur Windows (Pr√©paration)
Lancez le script de bridge sur votre machine h√¥te :
1. Installez les requis : `pip install flask opencv-python`.
2. Lancez le script : `python windows/camera_bridge.py`.
   *Ce script cr√©e un flux MJPEG sur le port 5002 de Windows.*

### 2. Trouver votre IP Windows
Dans un terminal Windows (PowerShell/CMD), tapez `ipconfig`. Cherchez l'IP de votre carte WiFi ou Ethernet (ex: `192.168.1.15`).

### 3. Lancer l'application dans WSL
```bash
# Remplacez <IP> par votre adresse IP Windows
./run_app.sh http://<IP>:5002/video_feed
```

Une fois lanc√©, ouvrez votre navigateur sur `http://localhost:5000` pour voir les r√©sultats.

---

## üõ†Ô∏è Ancienne m√©thode (Native usbipd)
*Uniquement si vous avez compil√© votre propre noyau WSL avec support UVC.*
"### 2. Sous WSL (Linux) - R√©solution de probl√®mes"
Si `ls /dev/video*` ne renvoie rien apr√®s l'attachement, c'est que votre noyau WSL (Kernel) manque de drivers UVC.

**Solution 1 (Recommand√©e) :**
Dans un PowerShell Windows (Admin) :
```powershell
wsl --update
wsl --shutdown
```
Relancez ensuite WSL. Les noyaux r√©cents (6.6+) supportent souvent les cam√©ras par d√©faut.

**Solution 2 (Secours) : Bridge R√©seau**
Si le driver bloque toujours, utilisez le script `windows_camera_bridge.py` fourni :
1. Sur **Windows** : `pip install flask opencv-python`
2. Sur **Windows** : `python windows_camera_bridge.py`
3. Sur **WSL** : `./run_app.sh http://<IP_VOTRE_PC>:5002/video_feed`

### 3. Lancer l'application
Une fois la cam√©ra d√©tect√©e :

---

## üìÇ Gestion des fichiers et GitHub
### Structure des mod√®les
Le dossier `models/` est mont√© depuis l‚Äôh√¥te et doit conserver cette arborescence claire :

| Sous-r√©pertoire | Contenu attendu |
|-----------------|-----------------|
| `models/pt/` | Poids YOLO originaux en `.pt` t√©l√©charg√©s via `prepare_models.py` ou `YOLO_PREPARE`, utilis√©s par `export_yolos_to_trt.py`. |
| `models/onnx/` | Faisceaux `.onnx` g√©n√©r√©s pour tous les mod√®les (YOLO et densit√©). Les scripts d√©placent les exports Achim du cache Ultralytics vers ce dossier. |
| `models/tensorrt/` | Moteurs TensorRT `.engine` compil√©s pour YOLO (batch 32) et LWCC (batch 8). Conversion pilot√©e par `convert_onnx_to_trt.py`. |
| `models/openvino/` | IR OpenVINO (`.xml` + `.bin`) produits par `convert_pth_to_openvino.py`. |
| `models/lwcc_weights/` | Pths LWCC persistants. `LWCC_WEIGHTS_PATH` pointe vers ce dossier, donc les t√©l√©chargements n‚Äôatterrissent plus dans `/.lwcc/weights`. |

Les scripts `prepare_models.py`, `download_lwcc_weights.py` et `export_yolos_to_trt.py` font maintenant en sorte de cr√©er ces dossiers avec des permissions 775, d‚Äôex√©cuter les conversions √† partir de la racine du d√©p√¥t, puis de nettoyer les sous-arborescences temporaires (`models/models/`). Il suffit de relancer `./run_app.sh` (ou `python3 prepare_models.py`) apr√®s toute mise √† jour pour reg√©n√©rer les poids aux bons emplacements.

