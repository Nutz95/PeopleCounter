# Docker : Build et Ex√©cution GPU (Ubuntu 24.04 + CUDA 13.1)

Ce d√©p√¥t contient un `Dockerfile` multi-stage optimis√© pour construire une image performante incluant **OpenCV 4.13.0 (CUDA)**, **PyTorch 2.9.1**, et **TensorRT 10.14**.

## üèóÔ∏è Proc√©dure de Build

Le build est optimis√© via un syst√®me multi-stage (OpenCV -> D√©pendances -> Runtime).

```bash
# Lancer le build (inclut la gestion du cache et le backup automatique)
./build_image.sh
```

### üîç V√©rification du build
Une fois l'image cr√©√©e, v√©rifiez que le GPU est bien accessible :
```bash
docker run --rm --gpus all people-counter:gpu-final python3 -c "import cv2; import torch; print('OpenCV CUDA:', cv2.cuda.getCudaEnabledDeviceCount()); print('PyTorch CUDA:', torch.cuda.is_available())"
```

## üß± Architecture CPU / GPU
```mermaid
flowchart LR
   subgraph Host[H√¥te Docker]
      CameraUSB -->|USB| CaptureService(Capture Process)
      CameraIP -->|RTSP/HLS| CaptureService
      CaptureService -->|M√©tadonn√©es| Pipeline
   end

   subgraph Pipeline["Application (containeur)"]
      Pipeline --> CPUQueue["CPU Pr√©-traitement"]
      CPUQueue --> GPUInfer["TensorRT YOLO / LWCC"]
      GPUInfer --> CPUDraw["CPU Fusion / Dessin"]
      CPUDraw --> WebUI["Web UI Metrics"]
   end

   subgraph Devices[Mat√©riel]
      GPU["GPU (CUDA, TensorRT)"]
      CPU["CPU (Linux host)"]
      GPUInfer --> GPU
      CPUQueue --> CPU
      CPUDraw --> CPU
      WebUI -->|HTTP| Browser
   end

   style GPUInfer stroke:#b83232,stroke-width:3px
   style GPU fill:#b83232,color:#fff
   style CPU fill:#2d5f8b,color:#fff
```

## üéûÔ∏è Flux vid√©o par tuiles
```mermaid
sequenceDiagram
   participant C as Capture
   participant P as Pipeline
   participant Y as YOLO Seg
   participant GPU as GPU
   participant D as Draw/UI

   C->>P: frame
   P->>Y: split into tiles
   Y->>Y: t_crop (CPU)
   Y->>GPU: t_inf (TensorRT)
   Y->>Y: t_fuse & t_draw (CPU)
   Y->>P: merged masks
   P->>D: overlay + metrics (includes t_inf, t_draw, total_internal)
```

## üöÄ Ex√©cution de l'application

Utilisez le script d'ex√©cution qui g√®re automatiquement les acc√®s GPU, cam√©ras et ports r√©seaux.

```bash
# Lancer l'application (utilise /dev/video0 par d√©faut)
./run_app.sh

# Pour utiliser un autre p√©riph√©rique cam√©ra
./run_app.sh /dev/video1
```

---

## üì∏ Cam√©ra USB sur WSL2 (Windows)

Puisque le noyau WSL2 par d√©faut ne supporte pas les cam√©ras USB nativement (pas de `/dev/video*`), nous utilisons un **Bridge Vid√©o** pour envoyer le flux de Windows vers Docker.

### 1. Sur Windows (Pr√©paration)
Lancez le script de bridge sur votre machine h√¥te :
1. Installez les requis : `pip install flask opencv-python`.
2. Lancez le script : `python windows/camera_bridge.py`.
   *Ce script cr√©e un flux MJPEG sur le port 5002 de Windows.*

### 2. Trouver votre IP Windows
Dans un terminal Windows (PowerShell/CMD), tapez `ipconfig`. Cherchez l'IP de votre carte WiFi ou Ethernet (ex: `192.168.1.15`).

### 3. Lancer l'application dans WSL
```bash
# Remplacez <IP> par votre adresse IP Windows
./run_app.sh http://<IP>:5002/video_feed
```

Une fois lanc√©, ouvrez votre navigateur sur `http://localhost:5000` pour voir les r√©sultats.

---

## üõ†Ô∏è Ancienne m√©thode (Native usbipd)
*Uniquement si vous avez compil√© votre propre noyau WSL avec support UVC.*
"### 2. Sous WSL (Linux) - R√©solution de probl√®mes"
Si `ls /dev/video*` ne renvoie rien apr√®s l'attachement, c'est que votre noyau WSL (Kernel) manque de drivers UVC.

**Solution 1 (Recommand√©e) :**
Dans un PowerShell Windows (Admin) :
```powershell
wsl --update
wsl --shutdown
```
Relancez ensuite WSL. Les noyaux r√©cents (6.6+) supportent souvent les cam√©ras par d√©faut.

**Solution 2 (Secours) : Bridge R√©seau**
Si le driver bloque toujours, utilisez le script `windows_camera_bridge.py` fourni :
1. Sur **Windows** : `pip install flask opencv-python`
2. Sur **Windows** : `python windows_camera_bridge.py`
3. Sur **WSL** : `./run_app.sh http://<IP_VOTRE_PC>:5002/video_feed`

### 3. Lancer l'application
Une fois la cam√©ra d√©tect√©e :

---

## üìÇ Gestion des fichiers et GitHub

### Fichiers obsol√®tes (√† supprimer)
Les fichiers suivants sont des reliquats d'anciennes versions et ne sont plus n√©cessaires avec le nouveau `Dockerfile` :
- `Dockerfile.probe` : Test temporaire.
- `setup.sh`, `run_docker.sh`, `setup_docker.sh` : Remplac√©s par le workflow Docker standard.
- `make_wheelhouse.sh` (racine) : Utilisez `scripts/make_wheelhouse.sh`.

### Que faut-il commiter ?
- **OUI** : `Dockerfile`, `requirements.cuda.txt`, `scripts/make_wheelhouse.sh`.
- **NON** : Le dossier `wheelhouse/` (trop lourd, contient des binaires `.whl` qui sont t√©l√©charg√©s dynamiquement durant le build Docker via le cache).
- **NON** : Les dossiers `models/` (doivent √™tre g√©r√©s via un script de t√©l√©chargement ou stock√©s s√©par√©ment).

### Structure des mod√®les
Le dossier `models/` est mont√© depuis l‚Äôh√¥te et doit conserver cette arborescence claire :

| Sous-r√©pertoire | Contenu attendu |
|-----------------|-----------------|
| `models/pt/` | Poids YOLO originaux en `.pt` t√©l√©charg√©s via `prepare_models.py` ou `YOLO_PREPARE`, utilis√©s par `export_yolos_to_trt.py`. |
| `models/onnx/` | Faisceaux `.onnx` g√©n√©r√©s pour tous les mod√®les (YOLO et densit√©). Les scripts d√©placent les exports Achim du cache Ultralytics vers ce dossier. |
| `models/tensorrt/` | Moteurs TensorRT `.engine` compil√©s pour YOLO (batch 32) et LWCC (batch 8). Conversion pilot√©e par `convert_onnx_to_trt.py`. |
| `models/openvino/` | IR OpenVINO (`.xml` + `.bin`) produits par `convert_pth_to_openvino.py`. |
| `models/lwcc_weights/` | Pths LWCC persistants. `LWCC_WEIGHTS_PATH` pointe vers ce dossier, donc les t√©l√©chargements n‚Äôatterrissent plus dans `/.lwcc/weights`. |

Les scripts `prepare_models.py`, `download_lwcc_weights.py` et `export_yolos_to_trt.py` font maintenant en sorte de cr√©er ces dossiers avec des permissions 775, d‚Äôex√©cuter les conversions √† partir de la racine du d√©p√¥t, puis de nettoyer les sous-arborescences temporaires (`models/models/`). Il suffit de relancer `./run_app.sh` (ou `python3 prepare_models.py`) apr√®s toute mise √† jour pour reg√©n√©rer les poids aux bons emplacements.

```