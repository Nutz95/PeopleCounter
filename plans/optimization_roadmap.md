# Optimization Roadmap ‚Äî PeopleCounter app_v2
> Cr√©√© : 2026-02-18 | GPU : RTX 5060 Ti (Blackwell sm_120) | TRT : 10.15.1.29 | Cible : 4K@30FPS

---

## Contexte & baseline

### Clarifications architecturales
- **Format YOLO** : `images` ‚Üí NCHW FP32 (`[batch, 3, height, width]`). Ni kHWC ni kNV12 support√©s nativement. La conversion NV12‚ÜíRGB est obligatoire, mais fusionnable.
- **Engines actuels** : `yolo26n-seg-fp8-qdq.engine` (dynamic batch, FP8 Q/DQ), `yolo26n-seg-fp8-b32.engine` (batch32 FP8). Plus de variantes batch1/16/32 s√©par√©es ‚Äî profil dynamique.
- **Pipeline tiles** : 1 batch global (yolo_global) sur stream d√©di√© + 2 groupes parall√®les de ~16 tiles chacun (yolo_tiles, `YoloTilingParallelTRT`, 2 contexts TRT s√©par√©s).

### M√©triques baseline (38 tests, cam√©ra r√©elle, ASYNC_OVERLAY)
| M√©trique | Valeur | Budget | Statut |
|---|---|---|---|
| `end_to_end_ms` | **37.6 ms** | 33 ms | üî¥ +14% |
| `fusion_wait_ms` | **20.1 ms** | 8 ms | üî¥ +250% |
| `inference_model_yolo_tiles_ms` | 19.9 ms | 20 ms | üü¢ 99% |
| `inference_model_yolo_global_ms` | 3.6 ms | 15 ms | üü¢ |
| `preprocess_nv12_bridge_ms` | 8.7 ms | 16 ms | üü¢ |
| `nvdec_ms` | 1.3 ms | 42 ms | üü¢ |
| Tests passants | **42 / 42** | ‚Äî | ‚úÖ |

### Fichier de suivi des gains
‚Üí [optimization_gains.html](optimization_gains.html) (g√©n√©r√© automatiquement apr√®s chaque √©tape)

---

## Plan d'optimisation

### #1 ‚Äî Timing Cache persistant ‚úÖ PRIORIT√â IMM√âDIATE
**Effort** : ~1h | **Impact** : builds d√©terministes, rebuild 5-10√ó plus rapide

**Probl√®me** : `convert_onnx_to_trt.py` et `prepare_yolo_modelopt_fp8.py` ne persistent pas le timing cache TRT. Les tactics peuvent varier entre rebuilds sur sm_120 (FP8 Q/DQ).

**Impl√©mentation** :
- Modifier `convert_onnx_to_trt.py` : charger/sauvegarder `models/tensorrt/timing_cache.bin`
- Modifier `prepare_yolo_modelopt_fp8.py` : idem
- Ajouter classe `TimingCacheManager` dans `app_v2/infrastructure/`
- Tests : `test_timing_cache_manager.py` ‚Äî v√©rifier que le cache est cr√©√© et recharg√©

**R√©f√©rence TRT** : `sampleEditableTimingCache`, `demo/BERT/builder.py`

---

### #2 ‚Äî CUDA Graphs pour l'inf√©rence r√©p√©titive
**Effort** : 2-3j | **Impact estim√©** : ‚àí2 √† ‚àí5 ms sur `enqueue_ms` tiles

**Probl√®me** : `execute_async_v3()` re-soumet √† chaque frame le graphe complet de kernels CUDA ‚Üí overhead CPU non n√©gligeable sur tiles b=16 √† 30 FPS.

**Principe** : capturer le graphe une fois au warmup, puis `cudaGraphLaunch()` √† chaque frame (quasi-z√©ro overhead CPU).

**Contraintes** :
- N√©cessite des shapes fixes (incompatible avec dynamic batch en capture mode) ‚Üí on capture sur `opt_batch_size`
- La TRT execution context doit supporter un mode "graph_captured"
- Pas compatible avec re-shape mid-run ‚Üí fallback sur `execute_async_v3` si batch ‚â† captured

**Impl√©mentation** :
- Ajouter `CudaGraphCache` dans `app_v2/infrastructure/`
- √âtendre `TensorRTExecutionContext` avec option `enable_cuda_graphs: bool`
- Tests : `test_cuda_graph_execution.py` ‚Äî v√©rifier graph capture + launch + r√©sultats identiques

**R√©f√©rence TRT** : utilis√© dans les d√©mos Diffusion TRT, cuda-python APIs

---

### #3 ‚Äî AutoCast FP32‚ÜíFP16 mixte (ModelOpt AutoCast)
**Effort** : ~1j | **Impact** : alternative sans calibration, potentiellement meilleur que full-FP8 sur tiles

**Principe** : `modelopt.onnx.autocast.convert_to_mixed_precision()` identifie automatiquement les n≈ìuds sensibles (normalization, activations) et les garde en FP32, le reste en FP16. Pas besoin d'images de calibration.

**Int√©r√™t** : comparaison A/B FP8-QDQ vs FP16-mixte sur les tiles 640√ó640. Le FP16-mixte peut √™tre plus rapide si les Q/DQ nodes introduisent de l'overhead.

**Impl√©mentation** :
- Nouveau script `prepare_yolo_autocast_fp16.py` (similaire √† `prepare_yolo_modelopt_fp8.py`)
- Output : `models/tensorrt/yolo26n-seg-autocast-fp16.engine`
- Tests : `test_autocast_fp16_engine.py` ‚Äî v√©rifier que l'engine charge et produit des r√©sultats coh√©rents

**R√©f√©rence TRT** : `samples/python/strongly_type_autocast/`

---

### #4 ‚Äî Weight Stripping + Engine Refit
**Effort** : ~1j | **Impact** : engine disque ‚àí60% taille, refit √† chaud possible (A/B models)

**Principe** : engine "stripped" sans les poids ‚Üí d√©ploiement all√©g√©. Au d√©marrage, refit depuis le fichier `.onnx` original via `IParserRefitter` (~30 ms). Performances identiques.

**Int√©r√™t additionnel** : pouvoir swapper les poids sans rebuild complet de l'engine (ex: switch yolo26n ‚Üî yolo26m sans rebuild TRT).

**Impl√©mentation** :
- Modifier `convert_onnx_to_trt.py` : ajouter option `--weight-stripped`
- Cr√©er `TensorRTEngineRefitter` dans `app_v2/infrastructure/`
- Int√©grer dans `TensorRTEngineLoader.load()` : detect stripped engine ‚Üí refit auto
- Tests : `test_engine_refit.py` ‚Äî v√©rifier refit + inf√©rence coh√©rente

**R√©f√©rence TRT** : `samples/python/sample_weight_stripping/`

---

### #5 ‚Äî Kernel fusionn√© NV12‚ÜíRGB+Resize+NCHW *(le plus gros gain potentiel)*
**Effort** : 3-5j | **Impact estim√©** : **‚àí4 √† ‚àí8 ms** sur `preprocess_nv12_bridge_ms` (8.7 ms ‚Üí ~1-2 ms)

**Probl√®me actuel** :
1. `nv12_cuda_bridge.py` ‚Üí NV12 √† GPU ptr ‚Üí RGB HWC uint8 tensor (8.7 ms) 
2. `preprocess.py` ‚Üí letterbox/tiling ‚Üí NCHW FP16 tensor (0.005 ms, d√©j√† rapide)

Le goulot est l'√©tape 1. Un kernel fusionn√© ferait en **un seul pass** : NV12 plane ptr ‚Üí letterbox/tile ‚Üí NCHW FP16 normalis√© [0,1]. Z√©ro tensor interm√©diaire.

**Format YOLO confirm√©** : `images` NCHW FP32 (dtype=1). L'execution context castait en FP16 √† la vol√©e ‚Üí le kernel peut sortir directement en FP16.

**D√©pendances Docker** :
- `cuda-python` (d√©j√† pr√©sent depuis TRT 10.14 migration)
- Triton (`triton` package) ou compilation C extension PyTorch ‚Üí √† tester dans image
- Alternative pure PyTorch : `torch.ops.torchvision` ou custom kernel via `torch.utils.cpp_extension`

**Impl√©mentation** :
- √âcrire `app_v2/kernels/fused_nv12_preprocess.py` avec kernel Triton ou cuda-python
- Fallback sur `nv12_cuda_bridge` si kernel indisponible (d√©gradation gracieuse)
- Tests : `test_fused_nv12_preprocess.py` ‚Äî comparer output avec pipeline existant (MSE < Œµ)

---

### #6 ‚Äî IStreamWriter (s√©rialisation engine ‚Üí m√©moire/r√©seau)
**Effort** : ~0.5j | **Impact** : d√©ploiement sans I/O disque, chargement engine depuis RAM

**Principe** : `builder.build_serialized_network_to_stream()` + classe `IStreamWriter` custom ‚Üí s√©rialiser directement vers un buffer m√©moire ou un socket. Utile pour Docker sans volume `models/`.

**Impl√©mentation** :
- Cr√©er `app_v2/infrastructure/engine_stream_serializer.py`
- Modifier `TensorRTEngineLoader` pour accepter `bytes` en plus de path

**R√©f√©rence TRT** : `samples/python/stream_writer/`

---

## Ordre d'ex√©cution recommand√©

```
#1 Timing Cache     ‚Üí builds d√©terministes (pr√©requis pour comparer les suivants)
#3 AutoCast FP16    ‚Üí comparaison A/B vs FP8, sans risque
#4 Weight Stripping ‚Üí infrastructure propre pour la suite
#2 CUDA Graphs      ‚Üí optimisation runtime inference
#5 Kernel fusionn√©  ‚Üí plus gros impact, plus risqu√©, en dernier
#6 IStreamWriter    ‚Üí infrastructure, peut se faire en parall√®le
```

---

## Tableau de suivi des gains

> Mis √† jour apr√®s chaque √©tape. G√©n√©r√© dans `plans/optimization_gains.html`.
> Les m√©triques `end_to_end_ms` / `inference_tiles_ms` / `bridge_ms` n√©cessitent une cam√©ra r√©elle (NVDEC_TEST_STREAM_URL).

| # | Optimisation | Fichiers cr√©√©s / modifi√©s | Tests | `end_to_end_ms` | `inference_tiles_ms` | `bridge_ms` | Statut |
|---|---|---|---|---|---|---|---|
| 0 | **Baseline** | ‚Äî | 42 ‚úÖ | 37.6 ms üî¥ | 19.9 ms | 8.7 ms | ‚úÖ r√©f√©rence |
| 1 | **Timing Cache** | `timing_cache_manager.py`, `convert_onnx_to_trt.py`, `prepare_yolo_modelopt_fp8.py` | +14 ‚Üí 56 ‚úÖ | *(pas d'impact latence)* | *(pas d'impact latence)* | ‚Äî | ‚úÖ mesur√© |
| 2 | **CUDA Graphs** | `cuda_graph_cache.py`, `tensorrt_execution_context.py` | +8 ‚Üí 64 ‚úÖ | √† mesurer | √† mesurer | ‚Äî | ‚úÖ impl√©ment√© |
| 3 | **AutoCast FP16** | `prepare_yolo_autocast_fp16.py`, `2_prepare_nvdec.sh` | +7 ‚Üí 71 ‚úÖ | √† mesurer | √† mesurer | ‚Äî | ‚úÖ mesur√© (voir ci-dessous) |
| 4 | **Weight Stripping** | `engine_refitter.py`, `convert_onnx_to_trt.py` | +11 ‚Üí 82 ‚úÖ | *(perf = FP32)* | *(perf = FP32)* | ‚Äî | ‚úÖ mesur√© (voir ci-dessous) |
| 5 | **Fused NV12+letterbox** | `nv12_cuda_bridge.py`, `preprocess.py` | +7 ‚Üí 89 ‚úÖ | √† mesurer | √† mesurer | üéØ -4 ms vis√© | ‚úÖ impl√©ment√© |
| 6 | **IStreamWriter** | `engine_stream_writer.py`, `convert_onnx_to_trt.py` | +12 ‚Üí 101 ‚úÖ | ‚Äî | ‚Äî | ‚Äî | ‚úÖ impl√©ment√© |

**Total tests** : 42 (baseline) ‚Üí **101** (+59 nouveaux tests)

---

## Benchmarks moteur ‚Äî `trtexec` batch=8 (RTX 5060 Ti, 2026-02-18)

> Commande : `trtexec --loadEngine=<engine> --shapes=images:8x3x640x640 --warmUp=200 --iterations=100 --avgRuns=10`

| Engine | Taille | GPU median | Latency median | Œî GPU vs FP32 | Œî Taille vs FP32 | Notes |
|--------|--------|-----------|----------------|---------------|-------------------|-------|
| `yolo26n-seg.engine` (FP32 baseline) | **7.8 MB** | **5.17 ms** | **8.16 ms** | ref | ref | Profil dynamique batch 1‚Äì32 |
| `yolo26n-seg-fp16-mixed.engine` (opt #3) | 7.9 MB | **4.89 ms** | **7.88 ms** | **‚àí5.4 %** | +1 % | FP16 builder flag (ModelOpt fallback) |
| `yolo26n-seg-fp8-qdq.engine` (FP8 QDQ) | **6.5 MB** | 5.10 ms | 8.19 ms | ‚àí1.4 % | **‚àí17 %** | Profil dynamique, Q/DQ nodes |
| `yolo26n-seg-fp8-b32.engine` (FP8 b32) | 7.8 MB | 5.14 ms | 8.16 ms | ‚àí0.6 % | 0 % | Batch statique 32 |
| `yolo26n-seg-stripped.engine` (opt #4) | **4.1 MB** | ~5.17 ms | ~8.16 ms | 0 % | **‚àí47 %** | Poids recharg√©s via refit (514 ms d√©marrage) |

**Observations** :
- FP16 donne le meilleur gain GPU compute (‚àí5.4 %) malgr√© la limitation ModelOpt (fallback FP16 flag) ‚Äî moteur 7.9 MB l√©g√®rement plus gros que FP32 (overhead STRONGLY_TYPED non utilis√©)
- FP8-QDQ est 17 % plus petit mais sans gain compute significatif sur RTX 5060 Ti (sm_120) ‚Äî le Q/DQ overhead annule le b√©n√©fice pour ce mod√®le tiny
- Weight stripping r√©duit la taille de ‚àí47 % avec performances identiques ; seul co√ªt : 514 ms de refit au premier d√©marrage

---

## Commandes de r√©f√©rence

```bash
# Tests complets (baseline)
./5_run_tests.sh --app-version v2

# Ou directement dans Docker
./docker_exec.sh python -m pytest app_v2/tests/ -v

# Rapport e2e avec cam√©ra (si NVDEC_TEST_STREAM_URL d√©fini)
NVDEC_TEST_STREAM_URL=rtsp://... ./docker_exec.sh python -m pytest \
  app_v2/tests/integration/pipeline/test_pipeline_metrics_integration.py \
  -v -s

# trtexec benchmark engine actuel
./docker_exec.sh trtexec \
  --loadEngine=models/tensorrt/yolo26n-seg-fp8-qdq.engine \
  --batch=16 --warmUp=500 --iterations=100 --avgRuns=10
```
