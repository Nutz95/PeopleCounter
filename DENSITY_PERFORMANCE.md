# DM-Count QNRF â€” Density Model Performance Benchmark

Hardware: **NVIDIA RTX 5060 Ti** (sm_120 Blackwell, 16 GB GDDR7, 448 GB/s)  
Framework: **TensorRT 10.15.1.29**, CUDA 13.1  
Model: **DM-Count QNRF** (VGG16 backbone, density head, output stride 8)  
Input frame: **3840 Ã— 2160** (4K)

---

## ğŸ“ Why latency scales linearly with pixel count

VGG16 is a pure feed-forward conv net with no dynamic routing.
Every layer is **memory-bandwidth bound** on this GPU:
each kernel reads the input feature-map from DRAM, does compute,
and writes the output feature-map back.
The RTX 5060 Ti's 448 GB/s bus is already saturated at batch=18 Ã— 640Ã—720 tiles.

**Consequence**: latency âˆ total pixels processed, regardless of configuration.

```
batch=1  Ã— 640Ã—720  =   460 800 px â†’  5.2 ms
batch=18 Ã— 640Ã—720  = 8 294 400 px â†’ 92 ms  (18 Ã— 5.2 â‰ˆ 93 ms âœ“ LINEAR)
batch=4  Ã— 1920Ã—1088= 8 355 840 px â†’ 92 ms  (same pixel budget)
batch=1  Ã— 3840Ã—2160= 8 294 400 px â†’ 92 ms  (full 4K, same budget)
```

**Why do 3 parallel CUDA streams give the same result as 1 large batch?**
Parallel streams only help when the GPU has *idle* compute or memory bandwidth.
At batch=18 the memory bus is 100% saturated. Adding parallel streams queues
extra work behind an already-full bottleneck â€” the wall-clock time is identical.
This is the streaming equivalent of adding more cars to a full motorway.

---

## ğŸ”¬ All measured configurations (FP16)

### A â€” 640 Ã— 720 tile engine (`dm_count_qnrf.engine`, max_batch=18)

| Batch | Tiles covered (4K) | Latency | fps-equiv | Notes |
|------:|-------------------|--------:|----------:|-------|
| 1     | 1 tile (3.5 % of frame) |  5.24 ms | 190.9 | partial frame, good for sparse scenes |
| 4     | 4 tiles (14 %)    | 20.43 ms |  48.9 | **â‰¤20 ms target âœ…** â€” covers ~4 zones |
| 6     | 6 tiles (21 %)    | 30.70 ms |  32.6 | â‰¤33 ms real-time budget |
| 18    | full frame (100 %) | 91.40 ms |  10.9 | **baseline**, all 6Ã—3 tiles |

### B â€” Parallel-stream experiment (640 Ã— 720, same engine)

| Strategy | Tiles | Latency | vs batch=18 | Notes |
|----------|------:|--------:|:-----------:|-------|
| 3 streams Ã— batch=6 | 18 | 91.98 ms | **1.00Ã—** | **no benefit** â€” GPU saturated |

### C â€” 1920 Ã— 1088 tile engine (`dm_count_qnrf_1920x1088.engine`, max_batch=4)

| Batch | Tiles covered (4K) | Latency | fps-equiv | Notes |
|------:|-------------------|--------:|----------:|-------|
| 4     | full frame (2Ã—2)  | 92.16 ms |  10.9 | same latency as B=18@640Ã—720, **no rescaling** âœ… |

### D â€” 1920 Ã— 1088, batch=1 (`dm_count_qnrf_1920x1088_b1.engine`)
*Matches: 4K â†’ downscale to 1920Ã—1088, infer as single frame*

| Batch | Resolution | Precision | Latency | fps-equiv | Notes |
|------:|-----------|:----------|--------:|----------:|-------|
| 1     | 1920Ã—1088  | FP16      | **22.90 ms** | 43.7 | 4Kâ†’1080p, single forward pass â€” **â‰¤33 ms âœ…** â€” **current production** |
| 1     | 1920Ã—1088  | FP8 QDQ   | **21.79 ms** | 45.9 | Quantized with UCF-QNRF (32 imgs), 1.05Ã— speedup â€” negligible gain |

> **FP8 note**: Only ~5% improvement over FP16 on RTX 5060 Ti Blackwell.  
> DM-Count (VGG16) is memory-bandwidth bound; FP8 halves weight storage but  
> the DRAM bandwidth is already the bottleneck â€” the compute savings are masked.  
> Production remains on the FP16 engine.

### E â€” 3840 Ã— 2160, batch=1 (`dm_count_qnrf_3840x2160.engine`)
*Matches: 4K native inference, simple crop (no resize â€” 3840Ã—2160 is already mod-16)*

| Batch | Resolution | Latency | fps-equiv | Notes |
|------:|-----------|--------:|----------:|-------|
| 1     | 3840Ã—2160  | **96.76 ms** | 10.3 | full 4K native, slightly slower than Â§C due to large-tensor cache pressure |

---

## ğŸ“Š Pixel-budget law (visual summary)

```
Total pixels â†’ Latency (FP16 on RTX 5060 Ti)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   460 800 px  (  b=1 Ã— 640Ã—720)   â†’   5.2 ms  ğŸŸ¢ fast
 2 088 960 px  (  b=4 Ã— 640Ã—720,
               or b=1 Ã— 1920Ã—1088) â†’  21â€“23 ms  ğŸŸ¢ â‰¤33ms âœ…  (FP16: 22.9 ms / FP8: 21.8 ms)
 3 110 400 px  (  b=6 Ã— 640Ã—720)   â†’  31   ms  ğŸŸ¡ â‰¤33ms âœ…
 8 294 400 px  (  b=18 Ã— 640Ã—720,  â†’  92â€“97 ms  ğŸ”´ too slow
               or b=4 Ã— 1920Ã—1088,
               or b=1 Ã— 3840Ã—2160)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Note: 4K native (3840Ã—2160) runs slightly slower than
      18 small tiles because L2 cache efficiency drops
      with very large spatial tensors.
```

---

## ğŸ—ºï¸ Preprocessing strategies (4K frame)

| Strategy | Engine | Preprocessing | Latency | Coverage | Quality |
|----------|--------|--------------|--------:|---------|---------|
| **6Ã—3 tiles** | 640Ã—720 b=18 | extract 18 crops + resize | 92 ms | 100 % | âŒ rescaled |
| **2Ã—2 tiles** | 1920Ã—1088 b=4 | extract 4 crops (native) | 92 ms | 100 % | âœ… native res |
| **Resize â†’ 1080p** | 1920Ã—1088 b=1 (FP16) | bilinear 4Kâ†’1920Ã—1088 | 22.9 ms | 100 % (lower res) | ğŸŸ¡ rescaled once â€” **production** |
| **Resize â†’ 1080p FP8** | 1920Ã—1088 b=1 (FP8 QDQ) | bilinear 4Kâ†’1920Ã—1088 | 21.8 ms | 100 % (lower res) | ğŸŸ¡ rescaled once â€” minimal gain vs FP16 |
| **4K native crop** | 3840Ã—2160 b=1 | crop 4K to mod-16 (none needed) | ~92 ms | 100 % | âœ… native res |
| **Partial 4 tiles** | 640Ã—720 b=4 | 4 crops in ROI | ~20 ms | ~14 % | âœ… zones of interest |

---

## ğŸ¯ Recommendations by use case

### Real-time < 20 ms, partial coverage
â†’ **640Ã—720, batch=4**: process the 4 highest-density zones of the frame.  
  Configure via `target_width: 640`, `target_height: 720` in `pipeline.yaml`;  
  the planner will generate the first 4 tiles (top-left 2Ã—2 region by default).

### Global count at ~23 ms (full-frame, lower resolution)
â†’ **1920Ã—1088, batch=1 FP16**: resize 4K â†’ 1920Ã—1088 once, single inference.  
  Acceptable for crowd estimation; some fine detail lost vs native resolution.  
  FP8 QDQ variant available (`dm_count_qnrf_1920x1088_b1-fp8-qdq.engine`) but provides  
  only ~5% speedup (21.8 ms vs 22.9 ms) â€” not worth the accuracy trade-off.

### Best quality density map, no time constraint
â†’ **2Ã—2 tiles at 1920Ã—1088** (current default): 4 native-resolution tiles,  
  no rescaling, full spatial detail preserved. ~92 ms.

### < 30 ms on full 4K frame â€” FP8 Result
â†’ **FP8-QDQ engine built** (`dm_count_qnrf_1920x1088_b1-fp8-qdq.engine`)  
  Calibrated on UCF-QNRF Train (32 images). Measured speedup: **1.05Ã—** (21.79 ms vs 22.90 ms FP16).  
  DM-Count is memory-BW bound on Blackwell; FP8 Tensor Cores donâ€™t fully exploit the   
  reduced weight footprint because DRAM bandwidth saturates first.  
  **Conclusion**: production remains on FP16 engine.

---

## ğŸ—ï¸ Engine inventory

| Engine file | Tile size | max_batch | Size | Latency | Status |
|-------------|-----------|----------:|-----:|--------:|--------|
| `dm_count_qnrf.engine` | 640Ã—720 | 18 | ~41 MB | 91.4 ms (b=18) | âœ… built |
| `dm_count_qnrf_1920x1088.engine` | 1920Ã—1088 | 4 | ~41 MB | 92.2 ms (b=4) | âœ… built |
| `dm_count_qnrf_1920x1088_b1.engine` | 1920Ã—1088 | 1 | 41.2 MB | **22.90 ms** (b=1) | âœ… built â€” **production** |
| `dm_count_qnrf_1920x1088_b1-fp8-qdq.engine` | 1920Ã—1088 | 1 | ~21 MB | **21.79 ms** (b=1) | âœ… built (1.05Ã— FP16) |
| `dm_count_qnrf_3840x2160.engine` | 3840Ã—2160 | 1 | 41.2 MB | **96.76 ms** (b=1) | âœ… built |

---

## ğŸ”¬ How to reproduce benchmarks

```bash
# All benchmarks run inside Docker (people-counter:gpu-final-nvdec)

# 640Ã—720, full batch sweep + 3-stream parallel test
docker run --rm --gpus all --shm-size=4g \
  -v "$PWD:/app" -w /app people-counter:gpu-final-nvdec \
  python3 prepare_density_models.py --skip-fp8 --benchmark-strategies

# 1920Ã—1088, batch=4 (current default)
docker run --rm --gpus all --shm-size=4g \
  -v "$PWD:/app" -w /app people-counter:gpu-final-nvdec \
  python3 prepare_density_models.py --skip-fp8 --tile-size 1920x1088

# 1920Ã—1088, batch=1 (4K â†’ 1080p single-frame)
docker run --rm --gpus all --shm-size=8g \
  -v "$PWD:/app" -w /app people-counter:gpu-final-nvdec \
  python3 prepare_density_models.py --skip-fp8 --tile-size 1920x1088 --max-batch 1

# 3840Ã—2160, batch=1 (full 4K native, no crop)
docker run --rm --gpus all --shm-size=8g \
  -v "$PWD:/app" -w /app people-counter:gpu-final-nvdec \
  python3 prepare_density_models.py --skip-fp8 --tile-size 3840x2160

# FP8 (UCF-QNRF calib available at /home/nra/lwcc/tests/dataset/UCF-QNRF_calib/)
docker run --rm --gpus all --shm-size=4g \
  -v "$PWD:/app" -v /home/nra/lwcc/tests/dataset/UCF-QNRF_calib:/calib:ro \
  -w /app people-counter:gpu-final-nvdec \
  python3 prepare_density_models.py --tile-size 1920x1088 --max-batch 1 \
  --calib-dir /calib
# Result: FP16 22.90 ms â†’ FP8 21.79 ms (1.05Ã—, negligible)
```

---

## ğŸ“ Notes on model architecture

- **Backbone**: VGG16 (5 pooling stages â†’ spatial stride 32)
- **Density head**: `reg_layer` (512â†’256â†’128 conv blocks) + `density_layer` (128â†’1 conv + Ã—2 bilinear) â†’ effective stride **8**
- **Output shape**: `(B, 1, H/8, W/8)` â€” sum Ã— density_scale â‰ˆ head count
- **Input normalisation**: ImageNet mean/std, input in [0, 1] fp32
- **Tile dimensions**: must be **multiples of 16** (VGG16 feature-map alignment)
  - 3840 = 240 Ã— 16 âœ…, 2160 = 135 Ã— 16 âœ… â†’ 4K native needs **no crop**
  - 1920 = 120 Ã— 16 âœ…, 1088 = 68 Ã— 16 âœ… (nearest mul-16 â‰¥ 1080)
